{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, itertools, pandas as pd, glob, sys\n",
    "from numpy import array\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "from dotmap import DotMap\n",
    "from os.path import join as pjoin, exists as pexists, basename, dirname\n",
    "from scipy.spatial import distance\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "sys.path.append(f'{PROJECT_ROOT}/self_super_reconst')\n",
    "\n",
    "my_basename = lambda s: os.path.splitext(basename(s))[0]\n",
    "chained = lambda l: list(itertools.chain(*l))\n",
    "identity = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths/Params\n",
    "results_path          = f'{PROJECT_ROOT}/results/'             # Experiments' output reconstructions\n",
    "imagenet_depth_folder = f'{PROJECT_ROOT}/data/imagenet_depth'  # Depth images extracted for ImageNet data\n",
    "cuda_dev_id           = 0                                      # CUDA device ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(cuda_dev_id)\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "use_cuda = os.environ['CUDA_VISIBLE_DEVICES'] != ''\n",
    "\n",
    "class WithDepth(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_master, dataset_depth_name, depth_only=False):\n",
    "        super(WithDepth, self).__init__()\n",
    "        self.dataset_master = dataset_master\n",
    "        self.depth_only = depth_only\n",
    "        self.depth_images_paths = natsorted(glob.glob(pjoin(imagenet_depth_folder, dataset_depth_name, '*')))\n",
    "        assert len(self.dataset_master) == len(self.depth_images_paths), \"Cannot resolve correspondence of rgb images and depth maps (diff length).\"\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # IMPORTANT: assume correspondence between the RGB images and their depth maps\n",
    "        img_rgb, class_label = self.dataset_master[index]\n",
    "        img_depth = array(Image.open(self.depth_images_paths[index]))\n",
    "        if self.depth_only:\n",
    "            img = Image.fromarray(img_depth)\n",
    "        else:\n",
    "            img_rgb = array(img_rgb)\n",
    "            img = Image.fromarray(dstack([img_rgb, img_depth[...,None]]))\n",
    "        return img, class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_master)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /net/mraid11/export/data/guyga/PycharmProjects/SelfSuperReconst_public/self_super_reconst/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /net/mraid11/export/data/guyga/PycharmProjects/SelfSuperReconst_public/self_super_reconst/lpips/weights/v0.1/vgg.pth\n",
      "Setting up [baseline] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Setting up [baseline] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "\u001b[1m\u001b[36m   >> Loading checkpoint: vgg16_depth_only_norm_within_img\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from self_super_reconst.utils import (MultiBranch, perceptual_loss_layer, \n",
    "                                      norm_depth_img, cprintm, cprintc, cprint1, \n",
    "                                      timeit)\n",
    "import torchvision.models as torchvis_models\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from self_super_reconst import lpips\n",
    "import torch\n",
    "\n",
    "class PerceptualDistance(nn.Module):\n",
    "    def __init__(self, net='alex', is_lpips=True):\n",
    "        super().__init__()\n",
    "        self.net = lpips.LPIPS(net=net, lpips=is_lpips)\n",
    "    \n",
    "    def forward(self, img_A, img_B):\n",
    "        imgs_in = [lpips.im2tensor(array(img)) for img in [img_A, img_B]]\n",
    "        if use_cuda:\n",
    "            imgs_in = [img.cuda() for img in imgs_in]\n",
    "        return self.net.forward(*imgs_in).cpu().detach().squeeze().numpy().item()\n",
    "    \n",
    "class DepthPerceptualDistance(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        bbn = torchvis_models.__dict__['vgg16'](pretrained=True)\n",
    "        bbn.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), \n",
    "            *bbn.features[1:])\n",
    "        ckpt_name = 'vgg16_depth_only_norm_within_img'\n",
    "\n",
    "        cprint1('   >> Loading checkpoint: {}'.format(ckpt_name))\n",
    "        state_dict_loaded = torch.load(f'{PROJECT_ROOT}/data/imagenet_rgbd/{ckpt_name}_best.pth.tar', map_location='cpu')['state_dict']\n",
    "        state_dict_loaded = { k.replace('module.', ''): v for k, v in state_dict_loaded.items() }\n",
    "        bbn.load_state_dict(state_dict_loaded)\n",
    "\n",
    "        branch_dict = {  # VGG16 Blocks  # selectedLayers = [3, 6, 10, 14, 18]\n",
    "                        # After maxpools\n",
    "                        'conv1': ['features.{}'.format(i) for i in range(5)],\n",
    "                        'conv2': ['features.{}'.format(i) for i in range(10)],\n",
    "                        'conv3': ['features.{}'.format(i) for i in range(17)],\n",
    "                        'conv4': ['features.{}'.format(i) for i in range(24)],\n",
    "                        'conv5': ['features.{}'.format(i) for i in range(31)],\n",
    "                    }\n",
    "\n",
    "        main_branch = branch_dict['conv5']\n",
    "        branch_dict = {layer: branch_module_list[-1] for layer, branch_module_list in branch_dict.items()}\n",
    "        self.feats_extractor = MultiBranch(bbn, branch_dict, main_branch, spatial_out_dims=None)\n",
    "    \n",
    "    def extract_feats(self, img):\n",
    "        img_norm = norm_depth_img(img)\n",
    "        if use_cuda:\n",
    "            img_norm = img_norm.cuda()\n",
    "        return self.feats_extractor(img_norm)\n",
    "    \n",
    "    def forward(self, img_A, img_B):\n",
    "        imgs_in = [norm_depth_img(to_tensor(img).unsqueeze(0)) for img in [img_A, img_B]]\n",
    "        if use_cuda:\n",
    "            imgs_in = [img.cuda() for img in imgs_in]\n",
    "        feats_list = [self.feats_extractor(img) for img in imgs_in]\n",
    "        return np.mean([perceptual_loss_layer(*feats_layer_AB).cpu().detach().squeeze().numpy().item() for feats_layer_AB in zip(*feats_list)])\n",
    "    \n",
    "    def forward_all(self, img_A_list, img_B):\n",
    "        img_batch = torch.stack([norm_depth_img(to_tensor(img_A)) for img_A in img_A_list])\n",
    "        img_B = norm_depth_img(to_tensor(img_B).unsqueeze(0))\n",
    "        if use_cuda:\n",
    "            img_batch = img_batch.cuda()\n",
    "            img_B = img_B.cuda()\n",
    "        feats_A_all = self.feats_extractor(img_batch)\n",
    "        feats_B = self.feats_extractor(img_B)\n",
    "        distances = []\n",
    "        for i in range(len(img_A_list)):\n",
    "            feats_A = [feats[i] for feats in feats_A_all]\n",
    "            distances.append(np.mean([perceptual_loss_layer(*feats_layer_AB).cpu().detach().squeeze().numpy().item() for feats_layer_AB in zip(feats_A, feats_B)]))\n",
    "        \n",
    "        return distances\n",
    "\n",
    "# === Distance metrics\n",
    "distance_metric_dict = {\n",
    "    'perceptual_alex_cal': PerceptualDistance('alex', True),\n",
    "    'perceptual_vgg_cal': PerceptualDistance('vgg', True),\n",
    "    'perceptual_alex_noncal': PerceptualDistance('alex', False),\n",
    "    'perceptual_vgg_noncal': PerceptualDistance('vgg', False),\n",
    "    'corr_dist': lambda x, y: distance.correlation(array(x).flatten(), array(y).flatten()),\n",
    "\n",
    "    'perceptual_vgg_noncal_depth': DepthPerceptualDistance(),\n",
    "}\n",
    "\n",
    "# === Distractors\n",
    "valdir = f'{PROJECT_ROOT}/data/imagenet/val'\n",
    "distractors_rgb = ImageFolder(valdir)\n",
    "distractors_depth = WithDepth(distractors_rgb, 'val_depth_on_orig_large_png_uint8', depth_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by perceptual similarity\n",
    "\n",
    "def reorder(file_paths, str_key='depth'):\n",
    "    return [fp for fp in file_paths if str_key not in my_basename(fp)] + [fp for fp in file_paths if str_key in my_basename(fp)]\n",
    "\n",
    "isgray = lambda img: array(img).ndim == 2\n",
    "\n",
    "NO_DEPTH, DEPTH, DEPTH_FROM_TESTAVG = range(3)\n",
    "\n",
    "eval_opts_defaults = dict(distance_metric_name='perceptual_alex_cal', eval_depth=NO_DEPTH, collapse_func='', n_way=2, repeats=100, rank=0)\n",
    "\n",
    "def release_cuda(distance_metric):\n",
    "    if isinstance(distance_metric, nn.Module):\n",
    "        distance_metric.cpu()\n",
    "    \n",
    "@timeit\n",
    "def eval(exp_name, eval_opts):\n",
    "    opts = DotMap(dict(eval_opts_defaults))\n",
    "    opts.update(eval_opts)\n",
    "\n",
    "    distance_metric = distance_metric_dict[opts.distance_metric_name]\n",
    "    if isinstance(distance_metric, nn.Module):\n",
    "        if use_cuda:\n",
    "            distance_metric.cuda()\n",
    "        distance_metric.eval()\n",
    "\n",
    "    collapse_func = getattr(np, opts.collapse_func) if opts.collapse_func else identity\n",
    "    \n",
    "    exp_path = pjoin(results_path, exp_name)\n",
    "    if not pexists(exp_path):\n",
    "        cprint('(!) Experiment not found: {}'.format(exp_path), 'red')\n",
    "        return\n",
    "    \n",
    "    if opts.eval_depth == DEPTH_FROM_TESTAVG:\n",
    "        paths_list = natsorted(glob.glob(pjoin(exp_path, 'depth_from_test_avg', '*')))\n",
    "    else:\n",
    "        paths_list = natsorted(glob.glob(pjoin(exp_path, 'test_avg', '*')))\n",
    "\n",
    "    if opts.eval_depth == NO_DEPTH:\n",
    "        paths_list = [p for p in paths_list if not my_basename(p).endswith('depth')]\n",
    "    else:\n",
    "        paths_list = [p for p in paths_list if my_basename(p).endswith('gray_depth')]\n",
    "\n",
    "    sbs_list = [lpips.load_image(p) for p in paths_list]\n",
    "        \n",
    "    n_chan = 1 if opts.eval_depth else 3\n",
    "    img_list_actual, img_list_pred = zip(*[(Image.fromarray(img[:, :112, :n_chan].squeeze()), Image.fromarray(img[:, 112:, :n_chan].squeeze())) for img in sbs_list])\n",
    "        \n",
    "    if opts.eval_depth:\n",
    "        assert isgray(img_list_actual[0]) and isgray(img_list_pred[0])\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        if opts.n_way > 0:\n",
    "            if opts.eval_depth:\n",
    "                distractor_list = distractors_depth\n",
    "            else:\n",
    "                distractor_list = distractors_rgb\n",
    "            results = []\n",
    "            results_rank = []\n",
    "            for img_actual, img_pred in zip(img_list_actual, img_list_pred):\n",
    "                results_img = []\n",
    "                results_img_rank = []\n",
    "                for _ in range(opts.repeats):\n",
    "                    cands = [img_actual] + [distractor_list[ii][0].resize(img_actual.size, Image.ANTIALIAS) for ii in np.random.permutation(len(distractor_list))[:opts.n_way - 1]]\n",
    "                        \n",
    "                    if getattr(distance_metric, \"forward_all\", None):\n",
    "                        distances = distance_metric.forward_all(cands, img_pred)\n",
    "                    else:\n",
    "                        distances = [distance_metric(img_cand, img_pred) for img_cand in cands]\n",
    "                    \n",
    "                    results_img_rank.append(np.argwhere(np.argsort(distances) == 0).flatten()[0] / (len(distances) - 1))\n",
    "                    results_img.append(np.argsort(distances)[0] == 0)\n",
    "\n",
    "                results_rank.append(np.mean(results_img_rank))\n",
    "                results.append(np.mean(results_img))\n",
    "            release_cuda(distance_metric)\n",
    "            return collapse_func(results), collapse_func(results_rank)\n",
    "        else:  # Pairwise evaluation\n",
    "            res = collapse_func([distance_metric(array(img_actual), array(img_pred)) for img_actual, img_pred in zip(img_list_actual, img_list_pred)])\n",
    "            release_cuda(distance_metric)\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub3_rgbd_noDE', 'sub3_rgb_only_noDE', 'sub3_depth_only_noDE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list = [x for x in glob.glob(results_path + '/*') if pexists(x + '/test_avg')]\n",
    "files_list_sorted = sorted(files_list, key=os.path.getctime)\n",
    "list(map(os.path.basename, files_list_sorted[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub3_rgbd_noDE', 'sub3_rgb_only_noDE', 'sub3_depth_only_noDE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_list = list(files_list_sorted)\n",
    "list(map(os.path.basename, exp_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few examples of evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_prog = []\n",
    "\n",
    "comparison_prog += [\n",
    "    dict(\n",
    "        exp_name=exp_name, \n",
    "        opts=chained([\n",
    "            [\n",
    "                dict(n_way=n_way, eval_depth=NO_DEPTH, distance_metric_name='perceptual_vgg_noncal', collapse_func='', rank=0, repeats=10),\n",
    "                dict(n_way=n_way, eval_depth=NO_DEPTH, distance_metric_name='perceptual_vgg_noncal', collapse_func='', rank=1, repeats=10),\n",
    "            ] for n_way in [5, 10]  # , 50, 100, 500, 1000]\n",
    "        ])\n",
    "    ) for exp_name in chained([\n",
    "        [\n",
    "            f'sub{sbj_num}_rgbd_noDE', \n",
    "            f'sub{sbj_num}_rgb_only_noDE',\n",
    "#             f'sub{sbj_num}_rgbd', \n",
    "#             f'sub{sbj_num}_rgb_only',\n",
    "#         ] for sbj_num in range(1,6)])\n",
    "        ] for sbj_num in [3]])\n",
    "    ]\n",
    "\n",
    "comparison_prog += [\n",
    "    dict(\n",
    "        exp_name=exp_name, \n",
    "        opts=chained([\n",
    "            [\n",
    "                dict(n_way=n_way, eval_depth=DEPTH, distance_metric_name='perceptual_vgg_noncal_depth', collapse_func='', rank=0, repeats=10),\n",
    "                dict(n_way=n_way, eval_depth=DEPTH, distance_metric_name='perceptual_vgg_noncal_depth', collapse_func='', rank=1, repeats=10),\n",
    "            ] for n_way in [5, 10]  # , 50, 100, 500, 1000]\n",
    "        ])\n",
    "    ) for exp_name in chained([\n",
    "        [\n",
    "            f'sub{sbj_num}_rgbd_noDE', \n",
    "            f'sub{sbj_num}_depth_only_noDE',\n",
    "#             f'sub{sbj_num}_rgb_only_noDE',  # (See decoder flag 'depth_from_rgb' which could make depth evaluation here possible)\n",
    "#             f'sub{sbj_num}_rgbd', \n",
    "#             f'sub{sbj_num}_depth_only',\n",
    "#             f'sub{sbj_num}_rgb_only',\n",
    "#         ] for sbj_num in range(1,6)])\n",
    "        ] for sbj_num in [3]])\n",
    "    ]\n",
    "\n",
    "# comparison_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load previously evaluated data and add new evaluations\n",
    "# df = pd.read_pickle('eval_results/eval.pkl')\n",
    "\n",
    "# Or create a new one\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>score</th>\n",
       "      <th>distance_metric_name</th>\n",
       "      <th>eval_depth</th>\n",
       "      <th>collapse_func</th>\n",
       "      <th>n_way</th>\n",
       "      <th>repeats</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub3_rgbd_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub3_rgbd_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub3_rgbd_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub3_rgbd_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub3_rgb_only_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sub3_rgb_only_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sub3_rgb_only_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub3_rgb_only_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub3_rgbd_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal_depth</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub3_rgbd_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal_depth</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sub3_rgbd_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal_depth</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sub3_rgbd_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal_depth</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sub3_depth_only_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal_depth</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sub3_depth_only_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal_depth</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sub3_depth_only_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal_depth</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sub3_depth_only_noDE</td>\n",
       "      <td></td>\n",
       "      <td>perceptual_vgg_noncal_depth</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                exp_name score         distance_metric_name  eval_depth  \\\n",
       "0         sub3_rgbd_noDE              perceptual_vgg_noncal           0   \n",
       "1         sub3_rgbd_noDE              perceptual_vgg_noncal           0   \n",
       "2         sub3_rgbd_noDE              perceptual_vgg_noncal           0   \n",
       "3         sub3_rgbd_noDE              perceptual_vgg_noncal           0   \n",
       "4     sub3_rgb_only_noDE              perceptual_vgg_noncal           0   \n",
       "5     sub3_rgb_only_noDE              perceptual_vgg_noncal           0   \n",
       "6     sub3_rgb_only_noDE              perceptual_vgg_noncal           0   \n",
       "7     sub3_rgb_only_noDE              perceptual_vgg_noncal           0   \n",
       "8         sub3_rgbd_noDE        perceptual_vgg_noncal_depth           1   \n",
       "9         sub3_rgbd_noDE        perceptual_vgg_noncal_depth           1   \n",
       "10        sub3_rgbd_noDE        perceptual_vgg_noncal_depth           1   \n",
       "11        sub3_rgbd_noDE        perceptual_vgg_noncal_depth           1   \n",
       "12  sub3_depth_only_noDE        perceptual_vgg_noncal_depth           1   \n",
       "13  sub3_depth_only_noDE        perceptual_vgg_noncal_depth           1   \n",
       "14  sub3_depth_only_noDE        perceptual_vgg_noncal_depth           1   \n",
       "15  sub3_depth_only_noDE        perceptual_vgg_noncal_depth           1   \n",
       "\n",
       "   collapse_func  n_way  repeats  rank  \n",
       "0                     5       10     0  \n",
       "1                     5       10     1  \n",
       "2                    10       10     0  \n",
       "3                    10       10     1  \n",
       "4                     5       10     0  \n",
       "5                     5       10     1  \n",
       "6                    10       10     0  \n",
       "7                    10       10     1  \n",
       "8                     5       10     0  \n",
       "9                     5       10     1  \n",
       "10                   10       10     0  \n",
       "11                   10       10     1  \n",
       "12                    5       10     0  \n",
       "13                    5       10     1  \n",
       "14                   10       10     0  \n",
       "15                   10       10     1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new evaluation requests\n",
    "data = { \n",
    "    **{ 'exp_name': [], 'score': [] }, \n",
    "    **{ k: [] for k in eval_opts_defaults} \n",
    "    }\n",
    "\n",
    "for comparison_prog_exp in comparison_prog:\n",
    "    for opts_dict_delta in comparison_prog_exp['opts']:\n",
    "        opts_dict = dict(eval_opts_defaults)\n",
    "        opts_dict.update(opts_dict_delta)\n",
    "        data['exp_name'].append(comparison_prog_exp['exp_name'])\n",
    "        for opt, opt_val in opts_dict.items():\n",
    "            data[opt].append(opt_val)\n",
    "        data['score'].append('')\n",
    "           \n",
    "df1 = pd.DataFrame.from_dict(data)\n",
    "\n",
    "df = pd.concat([df, df1]).drop_duplicates(subset=['exp_name'] + list(eval_opts_defaults.keys())).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "\u001b[1m\u001b[36msub3_rgbd_noDE\u001b[0m\n",
      "distance_metric_name    perceptual_vgg_noncal\n",
      "eval_depth                                  0\n",
      "collapse_func                                \n",
      "n_way                                       5\n",
      "repeats                                    10\n",
      "rank                                        0\n",
      "\u001b[36meval | 20.5 sec\u001b[0m\n",
      "====================\n",
      "\u001b[1m\u001b[36msub3_rgbd_noDE\u001b[0m\n",
      "distance_metric_name    perceptual_vgg_noncal\n",
      "eval_depth                                  0\n",
      "collapse_func                                \n",
      "n_way                                      10\n",
      "repeats                                    10\n",
      "rank                                        0\n",
      "\u001b[36meval | 36.6 sec\u001b[0m\n",
      "====================\n",
      "\u001b[1m\u001b[36msub3_rgb_only_noDE\u001b[0m\n",
      "distance_metric_name    perceptual_vgg_noncal\n",
      "eval_depth                                  0\n",
      "collapse_func                                \n",
      "n_way                                       5\n",
      "repeats                                    10\n",
      "rank                                        0\n",
      "\u001b[36meval | 17.8 sec\u001b[0m\n",
      "====================\n",
      "\u001b[1m\u001b[36msub3_rgb_only_noDE\u001b[0m\n",
      "distance_metric_name    perceptual_vgg_noncal\n",
      "eval_depth                                  0\n",
      "collapse_func                                \n",
      "n_way                                      10\n",
      "repeats                                    10\n",
      "rank                                        0\n",
      "\u001b[36meval | 36.1 sec\u001b[0m\n",
      "====================\n",
      "\u001b[1m\u001b[36msub3_rgbd_noDE\u001b[0m\n",
      "distance_metric_name    perceptual_vgg_noncal_depth\n",
      "eval_depth                                        1\n",
      "collapse_func                                      \n",
      "n_way                                             5\n",
      "repeats                                          10\n",
      "rank                                              0\n",
      "\u001b[36meval | 34.2 sec\u001b[0m\n",
      "====================\n",
      "\u001b[1m\u001b[36msub3_rgbd_noDE\u001b[0m\n",
      "distance_metric_name    perceptual_vgg_noncal_depth\n",
      "eval_depth                                        1\n",
      "collapse_func                                      \n",
      "n_way                                            10\n",
      "repeats                                          10\n",
      "rank                                              0\n",
      "\u001b[36meval | 1.2 min\u001b[0m\n",
      "====================\n",
      "\u001b[1m\u001b[36msub3_depth_only_noDE\u001b[0m\n",
      "distance_metric_name    perceptual_vgg_noncal_depth\n",
      "eval_depth                                        1\n",
      "collapse_func                                      \n",
      "n_way                                             5\n",
      "repeats                                          10\n",
      "rank                                              0\n",
      "\u001b[36meval | 31.0 sec\u001b[0m\n",
      "====================\n",
      "\u001b[1m\u001b[36msub3_depth_only_noDE\u001b[0m\n",
      "distance_metric_name    perceptual_vgg_noncal_depth\n",
      "eval_depth                                        1\n",
      "collapse_func                                      \n",
      "n_way                                            10\n",
      "repeats                                          10\n",
      "rank                                              0\n",
      "\u001b[36meval | 1.1 min\u001b[0m\n",
      "\u001b[35mDone\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "skip_indices = []\n",
    "for i, row in df.iterrows():\n",
    "    if row.score != '' or i in skip_indices: \n",
    "        continue\n",
    "    row.pop('score')\n",
    "    exp_name = row.pop('exp_name')\n",
    "    print('=' * 20)\n",
    "    cprint1(exp_name)\n",
    "    print('\\n'.join(str(row).split('\\n')[:-1]))\n",
    "    res = eval(exp_name, dict(row))\n",
    "    if res == None:\n",
    "        continue\n",
    "    if not isinstance(res, list) and type(res) != tuple:\n",
    "        res = res.round(3)\n",
    "    if type(res) == tuple:\n",
    "        filter_v = dict(row)\n",
    "        filter_v.update({'exp_name': exp_name, 'score': ''})\n",
    "        if dict(row)['rank'] == 0:\n",
    "            filter_v.update({'rank': 1})\n",
    "            j = df.index[(df[list(filter_v)] == pd.Series(filter_v)).all(axis=1)]\n",
    "            if len(j):\n",
    "                assert len(j) == 1\n",
    "                j = j[0]\n",
    "                df.at[j, 'score'] = res[1]\n",
    "                skip_indices.append(j)\n",
    "            res = res[0]\n",
    "        else:  # rank == 1\n",
    "            filter_v.update({'rank': 0})\n",
    "            j = df.index[(df[list(filter_v)] == pd.Series(filter_v)).all(axis=1)]\n",
    "            if len(j):\n",
    "                assert len(j) == 1\n",
    "                j = j[0]\n",
    "                df.at[j, 'score'] = res[0]\n",
    "                skip_indices.append(j)\n",
    "            res = res[1]\n",
    "\n",
    "    df.at[i, 'score'] = res\n",
    "cprintm('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to evaluation file\n",
    "df.to_pickle(f'{PROJECT_ROOT}/eval_results/my_eval.pkl')  # Can also override original pkl file"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2d7e43485fee3654f8de1b3ae2f7dd1c1cd06f559a8c41420777e499f1abd15"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('guyga-gpu-new': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
